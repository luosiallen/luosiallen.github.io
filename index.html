
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
<head>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-108114467-1', 'auto');
        ga('send', 'pageview');
    </script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="chenxu hu,thu,Chenxu Hu,chenxuhu,hu chenxu,Hu Chenxu,huchenxu,THU,tsinghua,ZJU,zju">
    <meta name="description" content="">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <link href='https://fonts.googleapis.com/css?family=Titillium Web' rel='stylesheet'>
    <meta charset="utf-8">
    <style type="text/css">
        /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 400;
        }
        
        heading {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 19px;
            font-weight: bold
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: bold;
        }

        strongred {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            color: 'red';
            font-size: 16px
        }

        sectionheading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
            font-weight: bold
        }

        tableheading {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: bold;
        }

        .circle{
            width: 240px; 
            height: 240px; 
            border-radius: 50%; 
            overflow: hidden;
        }

        .circle > img{
            width: 100%;
            height: 100%;
        }
    </style>
    <link rel="shortcut icon" type="image/png" href="images/avatar.jpg"/>
    <title>Simian Luo - THU</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
<table width="900" border="0" align="center" cellspacing="0" cellpadding="20">
    <tr>
        <td halign="center">
            <p align="center">
                <font size="6">Simian Luo&nbsp;&nbsp;&nbsp;骆思勉</font>
            </p>
        </td>
    </tr>
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle" style="text-align:justify">
                        <p>I am Simian Luo, 23, currently a second year M.S student in Artificial Intelligence at 
                            <a href="https://iiis.tsinghua.edu.cn/en/"> IIIS (Institute for Interdisciplinary Information Sciences)</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University (清华交叉信息研究院) </a>
                            advised by <a href="https://hangzhaomit.github.io/">Prof. Hang Zhao</a>. 
                            <p>  Previously, I received my B.E. in Data Science from <a href="https://sds.fudan.edu.cn">School of Data Science</a>, 
                            <a href="https://sds.fudan.edu.cn">Fudan University</a>, advised by <a href="http://yanweifu.github.io">Prof. Yanwei Fu</a>. 
                        </p>
                        <p>   
                            My research interest focus on Advanced Generative Model, Multi-Modal Generation and LLM Application. 
                            In specfic, I have researched in diffusion model, audio-visual generation, 3D generation,  and Augmentated LLM. 
                            <br>
                            Currently, I am focused on developing new advanced Generative Models that can accelerate the development of the AI Generated Content (AIGC) industry. 
                            <!-- I am especially interested in
                            multi-modal machine learning and audio & speech processing, 
                            including speech synthesis, audio-visual learning, and some novel tasks 
                            combining audio, vision, language and other modalities. 
                            My research vision is to enable machines to learn, reason and interact from multi-modal inputs, 
                            just like human beings. -->
                        </p>
        
                        <p align="center">
                            <!-- <a href="files/cv.pdf">CV</a> | -->
                            <a href="mailto:luosm22@mails.tsinghua.edu.cn">E-Mail</a> | 
                            <a href="https://scholar.google.com/citations?user=I4UjoWoAAAAJ&hl=zh-CN">Google Scholar</a> |
                            <a href="https://github.com/luosiallen">Github</a> 
                        </p>
                    </td>
                    <td width="100%" valign="top">
                    <!-- <div class="circ">
                        <img src="images/me.jpg" width="50%">
                    </div>     -->
                    <img src="images/me5.jpg" width="95%">
                    </td>
                </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <sectionheading>News</sectionheading>
                    </td>
                </tr>
            </table>
            <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                <colgroup>
                    <col width="15%">
                    <col width="85%">
                </colgroup>
                <tbody>
                <tr>
                    <td valign="top" align="center"><tableheading>Nov. 2023</tableheading></td>
                    <td>
                        <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                        <!-- An invited <a href="https://www.techbeat.net/talk-info?id=629">talk</a> at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, about Neural Dubber: Dubbing for Videos According to Scripts -->
                        Follow our latest work: <font color="red"><strong>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module.</strong></font> A training-free neural network-based acceleration module that can directly plugged into various finetuned SD and SD LoRAs.<strong>!!</strong>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="center"><tableheading>Oct. 2023</tableheading></td>
                    <td>
                        <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                        <!-- An invited <a href="https://www.techbeat.net/talk-info?id=629">talk</a> at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, about Neural Dubber: Dubbing for Videos According to Scripts -->
                        Follow our latest work: <font color="red"><strong>Latent Consistency Models: Synthesizing High-Resolution Images With Few-Step Inference.</strong></font> The next generation of generative models after latent diffusion models (LDMs) <strong>!!</strong>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="center"><tableheading>June. 2023</tableheading></td>
                    <td>
                        <!-- An invited <a href="https://www.techbeat.net/talk-info?id=629">talk</a> at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, about Neural Dubber: Dubbing for Videos According to Scripts -->
                        Oral Presentation at <a href="https://sightsound.org/"> CVPR 2023 Sight and Sound Workshop </a> about our latest work:
                        <br><strong>Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models.</strong>
                    </td>
                </tr>
                <!-- <tr>
                    <td valign="top" align="center"><tableheading>Oct. 2021</tableheading></td>
                    <td>One paper accepted to NeurIPS 2021!
                    </td>
                </tr> -->
                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <sectionheading>Publications</sectionheading>
                        <p style="font-size: 20px"> * indicates equal contribution</p>
                    </td>
                </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/lcm-lora.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://latent-consistency-models.github.io">
                            <heading><img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> LCM-LoRA: A Universal Stable-Diffusion Acceleration Module
                            </heading>
                        </a><br>
                            <strong>Simian Luo*</strong>, Yiqin Tan*, Suraj Patil†, Daniel Gu†, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, Hang Zhao<br>
                            (*:Leading Authors, †:Core Contributor)<br>
                            <em>Technical Report &emsp; 2023.11  <br></em>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/lcm.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://latent-consistency-models.github.io">
                            <heading><img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> Latent Consistency Models: Synthesizing High-Resolution Images With Few-Step Inference
                            </heading>
                        </a><br>
                            <strong>Simian Luo*</strong>, Yiqin Tan*, Longbo Huang†, Jian Li†, Hang Zhao†<br>
                            <em>Preprint &emsp;  2023.10 <br></em>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/diff-foley6.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://diff-foley.github.io">
                            <heading>Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models</heading>
                        </a><br>
                            <strong>Simian Luo</strong>, Chuanhao Yan, Chenxu Hu, Hang Zhao<br>
                            <em>NeurIPS 2023<br></em>
                    </td>
                </tr>
                
                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/chatdb.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://chatdatabase.github.io">
                            <heading>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</heading>
                        </a><br>
                            Chenxu Hu*, Jie Fu*, Chenzhuang Du, <strong>Simian Luo</strong>, Junbo Zhao, Hang Zhao<br>
                            <em>LLM @ IJCAI 2023<br></em>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/ImAR2.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://arxiv.org/abs/2303.14700">
                            <heading>Learning Versatile 3D Shape Generation with Improved AR Models</heading>
                        </a><br>
                            <strong>Simian Luo*</strong>, Xuelin Qian*, Yanwei Fu, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue<br>
                            <em>ICCV 2023<br></em>
                    </td>
                </tr>
                
                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/qs-craft2.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Hong_QS-Craft_Learning_to_Quantize_Scrabble_and_Craft_for__Conditional_ACCV_2022_paper.pdf">
                            <heading>QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation</heading>
                        </a><br>
                            Yuxin Hong, Xuelin Qian, <strong>Simian Luo</strong>, Guoguo Guo, Xiangyang Xue, Yanwei Fu<br>
                            <em>ACCV 2022<br></em>
                    </td>
                </tr>
                
                <!-- <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/cvc.jpg' alt="sym" width="100%"
                                                              style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://tinglok.netlify.app/files/cvc/">
                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                            <heading>CVC: Contrastive Learning for Non-parallel Voice Conversion</heading>
                        </a><br>
                            Tingle Li*, Yichen Liu*, <strong>Chenxu Hu*</strong>, Hang Zhao<br>
                            <em>Interspeech 2021<br></em>
                            <a href="https://arxiv.org/abs/2011.00782">paper</a> |
                            <a href="https://tinglok.netlify.app/files/cvc/">project</a> |
                            <a href="https://github.com/Tinglok/CVC">code</a>
                    </td>
                </tr> -->

                <!-- <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/fs2.png' alt="sym" width="100%"
                                                                  style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://speechresearch.github.io/fastspeech2/">
                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                            <heading>FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</heading>
                        </a><br>
                            Yi Ren*, <strong>Chenxu Hu*</strong>, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu<br>
                            <em>ICLR 2021<br></em>
                            <a href="https://arxiv.org/abs/2006.04558">paper</a> |
                            <a href="https://speechresearch.github.io/fastspeech2/">project</a> |
                            <a href="https://github.com/espnet/espnet/tree/master/espnet2/tts/fastspeech2">code</a>
                    </td>
                </tr> -->

            </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <sectionheading>Presentations & Talks</sectionheading>
                        </td>
                    </tr>
                </table>
                <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                    <colgroup>
                        <col width="15%">
                        <col width="85%">
                    </colgroup>
                    <tbody>
                    <!-- <tr>
                        <td valign="top" align="center"><tableheading>Jan. 2022</tableheading></td>
                        <td> Invited talk at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, "Neural Dubber: Dubbing for Videos According to Scripts" (<a href="https://www.techbeat.net/talk-info?id=629">Talk</a>)
                        </td>
                    </tr> -->
                    <tr>
                        <td valign="top" align="center"><tableheading>June 2023</tableheading></td>
                        <td> Invited paper talk at <a href="https://sightsound.org/">Sight and Sound Workshop</a>, CVPR 2023, "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models." 
                        </td>
                    </tr>
                    <!-- <tr>
                        <td valign="top" align="center"><tableheading>May 2021</tableheading></td>
                        <td> Poster presentation at <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a>, "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"
                        </td>
                    </tr> -->
                    </tbody>
                </table>


            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <sectionheading>Media Coverage</sectionheading>
                    </td>
                </tr>
            </table>
            <table class="news-table" width="94%" align="center" border="0" style="text-align: justify">
                <colgroup>
                    <col width="25%">
                    <col width="75%">
                </colgroup>
                <tbody>
                <tr>
                    <td valign="top" align="left"><tableheading>Slator</tableheading></td>
                    <td><a href="https://slator.com/neural-dubber-tiktok-company-bytedance-explores-automated-dubbing/">
                        Neural Dubber: TikTok Parent Company ByteDance Explores Automated Dubbing</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Jiqizhixin(机器之心) Blog</tableheading></td>
                    <td><a href="https://mp.weixin.qq.com/s/I4BNwN25JOJoTs8stzDqUg">
                        A Chinese news about Neural Dubber</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Microsoft Research Blog</tableheading></td>
                    <td><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/fastspeech-2-fast-and-high-quality-end-to-end-text-to-speech/">
                        FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Microsoft Azure AI Blog</tableheading></td>
                    <td><a href="https://techcommunity.microsoft.com/t5/azure-ai-blog/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911?from=timeline">
                        Neural Text to Speech extends support to 15 more languages with state-of-the-art AI quality</a>
                    </td>
                </tr>
                </tbody>
            </table> -->


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td>
                            <sectionheading>Affiliations</sectionheading>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table align="center">
                    <tbody>
                    <tr>
                        <td width="28%" align="center">
                            <a href="https://www.fudan.edu.cn/en/" target="_blank">
                                <img style="width:120px" src="images/fdu.png"></a>&nbsp &nbsp
                        </td>
                        <!-- <td width="28%" align="center">
                            <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank">
                                <img style="width:120px" src="images/msra.png"></a>&nbsp &nbsp
                        </td> -->
                        <td width="28%" align="center">
                            <a href="https://www.tsinghua.edu.cn/en/" target="_blank">
                                <img style="width:120px" src="images/thu.png"></a>&nbsp &nbsp
                        </td>
                        <!-- <td width="28%" align="center">
                            <a href="https://www.bytedance.com/en/" target="_blank">
                                <img style="width:120px" src="images/bytedance.png"></a>&nbsp &nbsp
                        </td> -->
                    </tr>
                    <tr>
                        <td width="28%" align="center"><font size="3">FDU<br>2018-2022</font></td>
                        <!-- <td width="28%" align="center"><font size="3">MSRA<br>2019-2020</font></td> -->
                        <td width="28%" align="center"><font size="3">THU<br>2022 - Present</font></td>
                        <!-- <td width="28%" align="center"><font size="3">ByteDance<br>2021 - present</font></td> -->
                    </tr>
                    </tbody>
                </table>

            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td><br>
                        <p align="right"><font size="2">
                            Template credits: <a href="https://changan.io/">Changan Chen</a>
                        </font></p></td>
                </tr>
                </tbody>
            </table> -->

        </td>
    </tr>
</table>
</body>
</html>
